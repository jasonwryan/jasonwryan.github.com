---
layout: post
title: "Chroot in LVM on LUKS on Raid: Arch Linux"
date: 2012-02-13 10:25
comments: true
categories: [archlinux]
---
![image](http://miromiro.com/Blog-images/chroot-lvm.jpg)

In [my previous post describing this setup](http://jasonwryan.com/blog/2012/02/11/lvm/ "Post on setting this system up")
I made the point that grub won’t be installed and that it is necessary
to `chroot` in to install grub on both drives. This is the procedure I use
to do that—and to perform any other maintenance that requires working
from a live environment.

Again, most of this information is on the 
[Arch Wiki chroot page](https://wiki.archlinux.org/index.php/Chroot "Arch wiki page"), I
am just going to fill in the detail around this setup: LVM on LUKS on
Raid1.

Once you have booted into your live environment, load the modules that
are required; in this case: `raid1`, `dm-mod` and `dm-crypt`.

Check that udev hasn’t helpfully read the superblock of your Raid drives
and assembled phantom arrays:

{% codeblock lang:sh %}
cat /proc/mdstat
{% endcodeblock %}

I find that there are a couple there generally assigned the names
<span class="file">/dev/md126</span> and <span class="file">/dev/md127</span>.
These need to be stopped before assembling the correct arrays:

{% codeblock lang:sh %}
mdadm --stop /dev/md12[67]
mdadm --assemble /dev/md0 /dev/sd[ab]1
mdadm --assemble /dev/md1 /dev/sd[ab]2
{% endcodeblock %}

You should now have both your arrays up and running. The next step is to
unlock your encrypted device:

{% codeblock lang:sh %}
cryptsetup luksOpen /dev/md1 cryptdisk
{% endcodeblock %}

After entering your passphrase, your device will be unlocked. Next, make
the logical volumes available, and then check they are correct:

{% codeblock lang:sh %}
vgchange --available y vgroup
lvscan
{% endcodeblock %}

At this point, you are ready to mount the devices and `chroot`:

{% codeblock lang:sh %}
mkdir /mnt/arch
mount /dev/mapper/vgroup-lvroot /mnt/arch
{% endcodeblock %}

The next steps are straight from the wiki:

{% codeblock lang:sh %}
cd /mnt/arch
mount -t proc proc proc/
mount -t sysfs sys sys/
mount -o bind /dev dev/
{% endcodeblock %}

Mount the other parts of the system that you need to use in your
recovery – in this case, I need my <span class="file">/boot</span> partition:

{% codeblock lang:sh %}
mount /dev/md0 boot/
{% endcodeblock %}

Now `chroot` to the device and define your shell:

{% codeblock lang:sh %}
chroot . /bin/bash
{% endcodeblock %}

The wiki has some good advice about customizing your prompt to reinforce
the fact that you are in a chroot:

{% codeblock lang:sh %}
export PS1="(chroot) $PS1"
{% endcodeblock %}

With everything mounted, it is just a matter of performing your
maintenance. To reinstall `grub`:

{% codeblock lang:sh %}
# grub
grub> find /grub/stage1
grub> device (hd0) /dev/sda
grub> root (hd0,0)
grub> setup (hd0)
grub> quit
{% endcodeblock %}

Repeat for <span class="file">/dev/sdb</span> and both your drives will be bootable in the event
that one fails.

With the maintenance accomplished, all that remains is to exit the
`chroot` and unmount the devices cleanly:

{% codeblock lang:sh %}
exit
umount {proc,sys,dev,boot...}
cd ..
umount arch/
reboot
{% endcodeblock %}

Simple.

Creative Commons image by
[MPD01605](http://www.flickr.com/photos/mpd01605/4152508668/ "Rescue Engine 6 on Flickr")
